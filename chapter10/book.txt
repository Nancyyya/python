CS229 Supplemental Lecture notesJohn Duchi1 Binary classificationIn binary classification problems, the target y can take on at only twovalues. In this set of notes, we show how to model this problem by lettingy ∈ {−1,+1}, where we say that y is a 1 if the example is a member of thepositive class and y = −1 if the example is a member of the negative class.We assume, as usual, that we have input features x ∈ Rn.As in our standard approach to supervised learning problems, we firstpick a representation for our hypothesis class (what we are trying to learn),and after that we pick a loss function that we will minimize. In binaryclassification problems, it is often convenient to use a hypothesis class of theform h(x) = θT x, and, when presented with a new example x, we classify itas positive or negative depending on the sign of θT x, that is, our predictedlabel issign(h(x)) = sign(θT x) where sign(t) =1 if t > 00 if t = 0−1 if t < 0.In a binary classification problem, then, the hypothesis h with parametervector θ classifies a particular example (x, y) correctly ifsign(θT x) = y or equivalently yθT x > 0. (1)The quantity yθT x in expression (1) is a very important quantity in binaryclassification, important enough that we call the valueyxT θthe margin for the example (x, y). Often, though not always, one interpretsthe value h(x) = xT θ as a measure of the confidence that the parameter1